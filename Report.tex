\documentclass[12pt, a4paper, oneside, final]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor, ulem, soulutf8, soul, fancyhdr, amsmath, amssymb, amsthm, svg, wrapfig, csvsimple, float, caption, subcaption}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec, hyperref, multicol, listings}
\usepackage[most]{tcolorbox}
\usepackage[makeroom]{cancel}
\usepackage[most]{tcolorbox}
\usepackage{tocloft, longtable, skak, stmaryrd, color}
\usepackage[framemethod = tikz]{mdframed}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}
\definecolor{light-gray}{gray}{0.95}

\renewcommand{\figurename}{}
\addto\captionsrussian{\renewcommand{\figurename}{}}
\captionsetup[table]{labelformat = empty}

\hypersetup{
	colorlinks,
	citecolor = pink,
	filecolor = pink,
	linkcolor = pink,
	urlcolor = pink
}

\newtcblisting{mylisting}{
	listing only,
	breakable,
	colback = backcolour,
	enhanced jigsaw,
	sharp corners,
	boxrule = 0pt,
	frame hidden,
	listing options = {
		mathescape,
		commentstyle = \color{codegreen},
		keywordstyle = \color{magenta},
		numberstyle = \tiny\color{codegray},
		stringstyle = \color{codepurple},
		basicstyle = \ttfamily\footnotesize,
		breakatwhitespace = false,
		breaklines = true,
		captionpos = b,
		keepspaces = true,
		numbers = left,
		numbersep = 5pt,
		showspaces = false,
		showstringspaces = false,
		showtabs = false,
		tabsize = 4,
		inputencoding = utf8,
		language = python
	}
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\everymath{\displaystyle}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №3}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Ким Станислав Евгеньевич
		\end{tabular}
	}
	\vspace*{30em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Решение задачи нелинейной регрессии}
	Часто решая задачу создания регрессионной модели мы сталкиваемся с тем, что по жизни очень немногие рассматриваемые функции оказываются не представимы в виде обобщенной линейной зависимости или полиномиальной некоторой конечной степени $k$.
	Такая же ситуация часто случается и с некоторым набором данных, который нужно как-то обобщить.
	Именно в таких случаях к нам на помощь приходит более частный случай регрессионного анализа~-- \textit{нелинейная регрессия}.

	Идея построения нелинейной регрессии как и в случае с полиномиальной заключается в том, чтобы найти математическую функцию, которая максимально точно описывает зависимость между независимой переменной и зависимой от нее.
	Например, для построения нелинейной регрессии можно использовать функции типа полинома, логарифмической или экспоненциальной зависимости.

	В целом весь процесс нахождения нелинейной регрессионной модели можно поделить на два этапа:
	\begin{itemize}
		\item Определить регрессионную модель $f(w, x)$, которая зависит от параметров $w = (w_{1}, ~ \ldots, ~ w_{W})$ и свободной переменной $x$.
		\item Решить задачу по нахождению минимума сумма квадратов регрессионных остатков:
		\[
			S = \sum\limits_{i = 1}^{m}{r_{i}^{2}}, ~ r_{i} = y_{i} - f(w, x_{i})
		\]
	\end{itemize}
	Однако, решая в лоб такую задачу, мы сталкиваемся с оптимизационной задачи нахождения параметров нелинейной регрессионной модели.
	Тут к нам и приходят на помощь различные методы нахождения, в том числе и рассматриваемые ниже: \textit{Gauss-Newton} и \textit{Powell Dog Leg}.
	\subsection*{Gauss-Newton}
	Напомним, что мы решаем следующую задачу: дана нелинейная модель $f(w, x)$, где $w \in \mathbb{R}^{n}$, тогда сумма квадратов регрессионных остатков высчитывается как
	\[
		S = \sum\limits_{i = 1}^{\texttt{sizeof}~X}{(f(w, x_{i}) - y_{i})^2} \to \mathrm{min}
	\]
	Итак, теперь введем некоторые новые объекты для решения задачи, пусть $w^{0} = (w_{0}^{0}, ~ w_{1}^{0}, ~ \ldots, ~ w^{t}_{p})$~-- начальное приближение, и
	\begin{align*}
		\gimel &= \left(\dfrac{\partial{f}}{\partial{w_{j}}}{(w^{t}, x_{i})}\right)_{(\texttt{sizeof}~X) \times p}~-~\text{Якобиан, или матрица первых производных} \\
		\vec{f_{t}} &= \left(f(w^{t}, x_{i})\right)_{(\texttt{sizeof}~X) \times 1}~-~\text{вектор значений}~f \\
		\eth_{t} &= \texttt{const}~-~\text{размера шага}
	\end{align*}
	Тогда, формула $t$-й итерации рассматриваемого метода будет высчитываться как
	\[
		w^{t + 1} \gets w^{t} - \eth_{t} \cdot \underbrace{\left(\gimel^{\mathrm{T}}_{t}\gimel_{t}\right)^{-1}\gimel_{t}^{\mathrm{T}}}_{\Psi}(\vec{f_{t}} - y),
	\] где $\Psi$~-- это псевдообратная матрица, или решение некоторой задачи многомерной линейной регрессии, где мы ищем такой вектор $\Psi$, что
	\[
		\left\|\gimel_{t}\Psi - (\vec{f_{t}} - y)\right\|^{2} \to \mathrm{min},
	\] где $y$~-- вектор правильных/настоящих ответов нашей модели.
	Получается, для решения задачи, мы, так называемую, \textit{невязку} пытаемся приблизить линейной комбинацией вектора из матрицы Якобиана так, что при следующем шаге итерации получить такой $w^{t + 1}$, который бы сократил нам расстояние невязки.
	Причем, заметим, что на каждом шаге, задача будет новой, так как $\gimel_{t}$ зависит от текущего приближения, чтобы решить задачу многомерной регрессии.
	% TODO: надо ли ещё информации?
	\subsubsection*{Исследования}
	% TODO: добавить исследования из T1.ipynb
	\subsection*{Powell Dog Leg}
	% TODO: добавить информации для dogleg.
	\subsubsection*{Исследования}
	\newpage
	\section*{BFGS}
	\textit{BFGS}, или Алгоритм Бройдена~- Флетчера~- Гольдфарба~- Шанно~-- это тоже оптимизационный итерационный алгоритм для нахождения локального экстремума для не представимых данных или функций в линейном/полиномиальном виде.

	Один из известных квазиньютоновских методов (то есть, тех, которые основаны на получении информации о кривизне функции).
	Тут следует сразу пояснить, что в квазиньютоновских методах для нахождения оптимальных параметров используется довольно медлительное определение \textit{гессиана} функции (или: матрица вторых производных).
	И вот тут данный алгоритм ускоряет работу на порядок: ибо он не явно каждый раз высчитывает матрицу, а лишь приближает к ней значения.

	Рассмотрим идею этого алгоритма.
	Пусть дана нам некоторая функция $f(\vec{x})$ и, как обычно, решаем задачу оптимизации нахождения $\operatorname*{argmin}_{\vec{x}}{f(\vec{x})}$.
	Пусть также $x_{i} = \{x_{i}^{0}, ~ x_{i}^{1}, ~ \ldots, ~ x_{i}^{n - 1}\}$, где $n$~-- размерность рассматриваемого пространства; $x_{0} \gets \textbf{INIT}$~--- начальная точка; $H_{0} = B_{0}^{-1}$~--- начальное приближение, где $B_{0}^{-1}$~-- обратный гессиан функции.
	Тогда:
	\begin{enumerate}[1)]
		\setcounter{enumi}{-1}
		\item Пусть $i$~-- текущий номер итерации алгоритма.
		\item Находим точку, в направлении которой будем производить поиск, она определяется следующим образом:
		\[
			p_{i} = -H_{i} \times \nabla{f_{i}}
		\]
		\item Вычисляем $x_{i + 1}$ через рекуррентное соотношение следующего вида:
		\[
			x_{i + 1} = x_{i} + \alpha \cdot p_{i},
		\] где $\alpha$~-- коэффициент, удовлетворяющий условиям Вольфа, которые, напомню, выглядят вот так:
		\begin{align*}
			f(x_{i} + \alpha \cdot p_{i}) &\leqslant f(x_{i}) + c_{1} \cdot \alpha \cdot \nabla{f^{T}_{k}p_{i}} \\
			\nabla{f(x_{i} + \alpha \cdot p_{i})^{T}p_{i}} &\geqslant c_{2} \cdot \nabla{f^{T}_{i}p_{i}}
		\end{align*}
		\item Теперь определим размер шага алгоритма после данной итерации и изменение градиента следующими соответствующими образами:
		\begin{align*}
			s_{i} &= x_{i + 1} - x_{i} \\
			y_{i} &= \nabla{f_{i + 1}} - \nabla{f_{i}}
		\end{align*}
		\item Наконец, обновим гессиан функции, зная, что $\mathbf{I}$~-- единичная матрица и $\lambda = \dfrac{1}{y_{i}^{\mathrm{T}}s_{i}}$:
		\[
			H_{i + 1} = \left(\mathbf{I} - \lambda s_{i}y_{i}^{\mathrm{T}}\right)H_{i}\left(\mathbf{I} - \lambda y_{i} s_{i}^{\mathrm{T}}\right) + \lambda s_{i} s_{i}^{\mathrm{T}}
		\]
	\end{enumerate}
	% TODO: надо ли ещё информации?
	\subsection*{Исследования}
	% TODO: добавить исследования для bfgs.
	\newpage
	\section*{L-BFGS}
	% TODO: добавить информации для l-bfgs.
	\subsection*{Исследования}
	% TODO: добавить исследования для l-bfgs.
\end{document}
