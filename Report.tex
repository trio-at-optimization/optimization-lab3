\documentclass[12pt, a4paper, oneside, final]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor, ulem, soulutf8, soul, fancyhdr, amsmath, amssymb, amsthm, svg, wrapfig, csvsimple, float, caption, subcaption}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec, hyperref, multicol, listings}
\usepackage[most]{tcolorbox}
\usepackage[makeroom]{cancel}
\usepackage[most]{tcolorbox}
\usepackage{tocloft, longtable, skak, stmaryrd, color}
\usepackage[framemethod = tikz]{mdframed}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}
\definecolor{light-gray}{gray}{0.95}

\renewcommand{\figurename}{}
\addto\captionsrussian{\renewcommand{\figurename}{}}
\captionsetup[table]{labelformat = empty}

\hypersetup{
	colorlinks,
	citecolor = pink,
	filecolor = pink,
	linkcolor = pink,
	urlcolor = pink
}

\newtcblisting{mylisting}{
	listing only,
	breakable,
	colback = backcolour,
	enhanced jigsaw,
	sharp corners,
	boxrule = 0pt,
	frame hidden,
	listing options = {
		mathescape,
		commentstyle = \color{codegreen},
		keywordstyle = \color{magenta},
		numberstyle = \tiny\color{codegray},
		stringstyle = \color{codepurple},
		basicstyle = \ttfamily\footnotesize,
		breakatwhitespace = false,
		breaklines = true,
		captionpos = b,
		keepspaces = true,
		numbers = left,
		numbersep = 5pt,
		showspaces = false,
		showstringspaces = false,
		showtabs = false,
		tabsize = 4,
		inputencoding = utf8,
		language = python
	}
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\everymath{\displaystyle}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №3}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Ким Станислав Евгеньевич
		\end{tabular}
	}
	\vspace*{30em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Решение задачи нелинейной регрессии}
	Часто решая задачу создания регрессионной модели мы сталкиваемся с тем, что по жизни очень немногие рассматриваемые функции оказываются не представимы в виде обобщенной линейной зависимости или полиномиальной некоторой конечной степени $k$.
	Такая же ситуация часто случается и с некоторым набором данных, который нужно как-то обобщить.
	Именно в таких случаях к нам на помощь приходит более частный случай регрессионного анализа~-- \textit{нелинейная регрессия}.

	Идея построения нелинейной регрессии как и в случае с полиномиальной заключается в том, чтобы найти математическую функцию, которая максимально точно описывает зависимость между независимой переменной и зависимой от нее.
	Например, для построения нелинейной регрессии можно использовать функции типа полинома, логарифмической или экспоненциальной зависимости.

	В целом весь процесс нахождения нелинейной регрессионной модели можно поделить на два этапа:
	\begin{itemize}
		\item Определить регрессионную модель $f(w, x)$, которая зависит от параметров $w = (w_{1}, ~ \ldots, ~ w_{W})$ и свободной переменной $x$.
		\item Решить задачу по нахождению минимума сумма квадратов регрессионных остатков:
		\[
			S = \sum\limits_{i = 1}^{m}{r_{i}^{2}}, ~ r_{i} = y_{i} - f(w, x_{i})
		\]
	\end{itemize}
	Однако, решая в лоб такую задачу, мы сталкиваемся с оптимизационной задачи нахождения параметров нелинейной регрессионной модели.
	Тут к нам и приходят на помощь различные методы нахождения, в том числе и рассматриваемые ниже: \textit{Gauss-Newton} и \textit{Powell Dog Leg}.
	\subsection*{Gauss-Newton}
	Напомним, что мы решаем следующую задачу: дана нелинейная модель $f(w, x)$, где $w \in \mathbb{R}^{m}$, тогда сумма квадратов регрессионных остатков высчитывается как
	\[
		S = \sum\limits_{i = 1}^{\texttt{sizeof}~X}{(f(w, x_{i}) - y_{i})^2} \to \mathrm{min}
	\]
	Итак, пусть $n = \texttt{sizeof}~X$ и введем некоторые новые объекты для решения задачи, пусть $w^{0} = (w_{0}^{0}, ~ w_{1}^{0}, ~ \ldots, ~ w^{0}_{m})$~-- начальное приближение, и
	\begin{align*}
		\gimel &= \left(\dfrac{\partial{f}}{\partial{w_{j}}}{(w^{\mathbf{i}}, x_{i})}\right)_{n \times m}~-~\text{Якобиан, или матрица первых производных} \\
		\vec{f_{\mathbf{i}}} &= \left(f(w^{\mathbf{i}}, x_{i})\right)_{n \times 1}~-~\text{вектор значений функции}~f \\
		\eth_{\mathbf{i}} &= \texttt{const}~-~\text{размера шага}
	\end{align*}
	Тогда, формула $\mathbf{i}$-й итерации рассматриваемого метода будет высчитываться как
	\[
		w^{\mathbf{i} + 1} \gets w^{\mathbf{i}} - \eth_{\mathbf{i}} \cdot \underbrace{\left(\gimel^{\mathrm{T}}_{\mathbf{i}}\gimel_{\mathbf{i}}\right)^{-1}\gimel_{\mathbf{i}}^{\mathrm{T}}}_{\beta}(\vec{f_{\mathbf{i}}} - y),
	\] где $\beta$~-- это псевдообратная матрица к матрице $\gimel_{\mathbf{i}}$, или решение некоторой задачи многомерной линейной регрессии, где мы ищем такой вектор $\beta$, что
	\[
		\left\|\gimel_{\mathbf{i}}\beta - (\vec{f_{\mathbf{i}}} - y)\right\|^{2} \to \mathrm{min},
	\] где $y$~-- вектор правильных/настоящих ответов нашей модели.
	Получается, для решения задачи, мы, так называемую, \textit{невязку} пытаемся приблизить линейной комбинацией вектора из матрицы Якобиана так, что при следующем шаге итерации получить такой $w^{\mathbf{i} + 1}$, который бы сократил нам расстояние невязки.
	Причем, заметим, что на каждом шаге, задача будет новой, так как $\gimel_{\mathbf{i}}$ зависит от текущего приближения, чтобы решить задачу многомерной регрессии.

	Заметим, что здесь, по алгоритму, мы видим достаточно очевидное ограничение: $m \geqslant n$, в ином случае для $\gimel_{\mathbf{i}}^{\mathrm{T}}\gimel_{\mathbf{i}}$ не будет существовать обратной матрицы и, в следствии, решения к уравнению.
	\subsubsection*{Исследования}
	% TODO: добавить исследования из T1.ipynb
	\subsection*{Powell Dog Leg}
	% TODO: добавить информации для dogleg.
	\subsubsection*{Исследования}
	\newpage
	\section*{BFGS}
	\textit{BFGS}, или Алгоритм Бройдена~- Флетчера~- Гольдфарба~- Шанно~-- это тоже оптимизационный итерационный алгоритм для нахождения локального экстремума для не представимых данных или функций в линейном/полиномиальном виде.

	Один из известных квазиньютоновских методов (то есть, тех, которые основаны на получении информации о кривизне функции).
	Тут следует сразу пояснить, что в квазиньютоновских методах для нахождения оптимальных параметров используется довольно медлительное определение \textit{гессиана} функции (или: матрица вторых производных).
	И вот тут данный алгоритм ускоряет работу на порядок: ибо он не явно каждый раз высчитывает матрицу, а лишь приближает к ней значения.

	Рассмотрим идею этого алгоритма.
	Пусть дана нам некоторая функция $f(\vec{x})$ и, как обычно, решаем задачу оптимизации нахождения $\operatorname*{argmin}_{\vec{x}}{f(\vec{x})}$.
	Пусть также $x_{i} = \{x_{i}^{0}, ~ x_{i}^{1}, ~ \ldots, ~ x_{i}^{n - 1}\}$, где $n$~-- размерность рассматриваемого пространства; $x_{0} \gets \textbf{INIT}$~--- начальная точка; $H_{0} = B_{0}^{-1}$~--- начальное приближение, где $B_{0}^{-1}$~-- обратный гессиан функции.
	Тогда:
	\begin{enumerate}[1)]
		\setcounter{enumi}{-1}
		\item Пусть $i$~-- текущий номер итерации алгоритма.
		\item Находим точку, в направлении которой будем производить поиск, она определяется следующим образом:
		\[
			p_{i} = -H_{i} \times \nabla{f_{i}}
		\]
		\item Вычисляем $x_{i + 1}$ через рекуррентное соотношение следующего вида:
		\[
			x_{i + 1} = x_{i} + \alpha \cdot p_{i},
		\] где $\alpha$~-- коэффициент, удовлетворяющий условиям Вольфа, которые, напомню, выглядят вот так:
		\begin{align*}
			f(x_{i} + \alpha \cdot p_{i}) &\leqslant f(x_{i}) + c_{1} \cdot \alpha \cdot \nabla{f^{T}_{k}p_{i}} \\
			\nabla{f(x_{i} + \alpha \cdot p_{i})^{T}p_{i}} &\geqslant c_{2} \cdot \nabla{f^{T}_{i}p_{i}}
		\end{align*}
		\item Теперь определим размер шага алгоритма после данной итерации и изменение градиента следующими соответствующими образами:
		\begin{align*}
			s_{i} &= x_{i + 1} - x_{i} \\
			y_{i} &= \nabla{f_{i + 1}} - \nabla{f_{i}}
		\end{align*}
		\item Наконец, обновим гессиан функции, зная, что $\mathbf{I}$~-- единичная матрица и $\lambda = \dfrac{1}{y_{i}^{\mathrm{T}}s_{i}}$:
		\[
			H_{i + 1} = \left(\mathbf{I} - \lambda s_{i}y_{i}^{\mathrm{T}}\right)H_{i}\left(\mathbf{I} - \lambda y_{i} s_{i}^{\mathrm{T}}\right) + \lambda s_{i} s_{i}^{\mathrm{T}}
		\]
	\end{enumerate}
	% TODO: надо ли ещё информации?
	\subsection*{Исследования}
	% TODO: добавить исследования для bfgs.
	\newpage
	\section*{L-BFGS}
	\textit{L-BFGS}, или BFGS с ограниченной памятью~-- это оптимизационный алгоритм, который аппроксимирует оригинальный алгоритм BFGS с использованием заданного ограниченного объема памяти.

	L-BFGS как BFGS использует приближенную оценку Гессиана, при этом в явном виде посчитав только один раз, а все остальные шаги лишь преобразовывая.
	Проблема: BFGS хранит всегда $n \times n$ приближение к обратному Гессиану.
	Решение: хранить несколько векторов, которые неявно представляют приближение, представляющие из себя историю последних $m$ обновлений положения $\vec{x}$ и градиента $\nabla{f(\vec{x})}$.
	При этом, $m$ обычно выбирается небольшим ($m < 10$).

	Рассмотрим идею этого алгоритма.
	Во многом она будет совпадать с предыдущим, поэтому пропустим обозначения и перейдем сразу алгоритму.
	\begin{enumerate}[1)]
		\setcounter{enumi}{-1}
		\item Пусть $\mathbf{i}$~-- текущий номер итерации алгоритма, возьмем $g_{\mathbf{i}} = \nabla{f(x_{\mathbf{i}})}$.
		\item Также находим точку, в направлении которой будем производить поиск:
		\[
			p_{\mathbf{i}} = -H_{\mathbf{i}} \times \nabla{f_{\mathbf{i}}}
		\]
		\item Пусть мы сохранили $m$ обновлений вида:
		\begin{align*}
			s_{\mathbf{i}} &= x_{\mathbf{i} + 1} - x_{\mathbf{i}} \\
			y_{\mathbf{i}} &= g_{\mathbf{i} + 1} - g_{\mathbf{i}}
		\end{align*}
		\item Определим $\rho_{\mathbf{i}} = \dfrac{1}{y_{\mathbf{i}}^{\mathrm{T}}s_{\mathbf{i}}}$ и $H_{\mathbf{i}}^{0}$~-- <<начальная>> аппроксимация обратного гессиана, с которого начинается наша оценка на $\mathbf{i}$-ой итерации.
		Теперь, наконец, основная оптимизация: мы хотим оптимизировать основную рекуррентную формулу.
		\begin{itemize}
			\item Для данного $\mathbf{i}$ определим $\{q_{\mathbf{i} - m}, ~ q_{\mathbf{i} - m + 1}, ~ \ldots, ~ q_{\mathbf{i}}\}$, где
			\begin{align*}
				q_{\mathbf{i}} &= g_{\mathbf{i}} \\
				q_{i} &= \left(\mathbf{I} - \rho_{i}y_{i}s_{i}^{\mathrm{T}}\right)q_{i + 1} ~ \forall i \setminus \mathbf{i}
			\end{align*}
			\item Тогда рекурсивный алгоритм вычисления $q_{i}$ от $q_{i + 1}$ состоит в том, чтобы определить $\alpha_{i} \gets \rho_{i}s_{i}^{\mathrm{T}}q_{i + 1}$ и $q_{i} = q_{i + 1} - \alpha_{i}y_{i}$.
			\item Определим также $\{z_{\mathbf{i} - m}, ~ z_{\mathbf{i} - m + 1}, ~ \ldots, ~ z_{\mathbf{i}}\}$, где $\forall i ~ z_{i} \gets H_{i}q_{i}$.
			\subitem Существует еще один рекурсивный алгоритм вычисления этих векторов, который заключается в определении $z_{\mathbf{i} - m} \gets H_{\mathbf{i}}^{0}q_{\mathbf{i} - m}$, а затем рекурсивно определить $\beta_{i} \gets \rho_{i}y_{i}^{\mathrm{T}}z_{i}$ и $z_{i + 1} = z_{i} + (\alpha_{i} - \beta_{i})s_{i}$. Значение $z_{\mathbf{i}}$ тогда~-- наше направление восхождения.
		\end{itemize}
		Таким образом, мы можем вычислить направление спуска следующим образом:
		\begin{mylisting}
			$q \gets \nabla{f_{\mathbf{i}}}$;
			for $i \in [\mathbf{i} - 1, ~ \mathbf{i} - 2, ~ \ldots, ~ \mathbf{i} - m]$ do
				$\alpha \gets \rho_{i}s_{i}^{\mathrm{T}}q$;
				$q \gets q - \alpha_{i}y_{i}$;
			end (for)
			$r \gets H_{\mathbf{i}}^{0}q$;
			for $i \in [k - m, ~ k - m + 1, ~ \ldots, ~ k - 1]$ do
				$\beta \gets \rho_{i}y_{i}^{\mathrm{T}}r$;
				$r \gets r + s_{i}(\alpha_{i} - \beta)$;
			end (for)
			return $r \equiv H_{\mathbf{i}}\nabla{f_{\mathbf{i}}}$;
		\end{mylisting}
		\item Положим $H_{\mathbf{i}}^{0} = \gamma_{\mathbf{i}}\mathbf{I}$ следующим образом:
		\[
			\gamma_{\mathbf{i}} = \dfrac{s_{\mathbf{i} - 1}^{\mathrm{T}}y_{\mathbf{i} - 1}}{y_{\mathbf{i} - 1}^{\mathrm{T}}y_{\mathbf{i} - 1}}
		\]
	\end{enumerate}
	% TODO: добавить больше информации.
	\subsection*{Исследования}
	% TODO: добавить исследования для l-bfgs.
\end{document}
