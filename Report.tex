\documentclass[12pt, a4paper, oneside, final]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor, ulem, soulutf8, soul, fancyhdr, amsmath, amssymb, amsthm, svg, wrapfig, csvsimple, float, caption, subcaption}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec, hyperref, multicol, listings}
\usepackage[most]{tcolorbox}
\usepackage[makeroom]{cancel}
\usepackage[most]{tcolorbox}
\usepackage{tocloft, longtable, skak, stmaryrd, color}
\usepackage[framemethod = tikz]{mdframed}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}
\definecolor{light-gray}{gray}{0.95}

\renewcommand{\figurename}{}
\addto\captionsrussian{\renewcommand{\figurename}{}}
\captionsetup[table]{labelformat = empty}

\hypersetup{
	colorlinks,
	citecolor = pink,
	filecolor = pink,
	linkcolor = pink,
	urlcolor = pink
}

\newtcblisting{mylisting}{
	listing only,
	breakable,
	colback = backcolour,
	enhanced jigsaw,
	sharp corners,
	boxrule = 0pt,
	frame hidden,
	listing options = {
		mathescape,
		commentstyle = \color{codegreen},
		keywordstyle = \color{magenta},
		numberstyle = \tiny\color{codegray},
		stringstyle = \color{codepurple},
		basicstyle = \ttfamily\footnotesize,
		breakatwhitespace = false,
		breaklines = true,
		captionpos = b,
		keepspaces = true,
		numbers = left,
		numbersep = 5pt,
		showspaces = false,
		showstringspaces = false,
		showtabs = false,
		tabsize = 4,
		inputencoding = utf8,
		language = python
	}
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\everymath{\displaystyle}

\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №3}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Ким Станислав Евгеньевич
		\end{tabular}
	}
	\vspace*{30em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Решение задачи нелинейной регрессии}
	Часто решая задачу создания регрессионной модели мы сталкиваемся с тем, что по жизни очень немногие рассматриваемые функции оказываются не представимы в виде обобщенной линейной зависимости или полиномиальной некоторой конечной степени $k$.
	Такая же ситуация часто случается и с некоторым набором данных, который нужно как-то обобщить.
	Именно в таких случаях к нам на помощь приходит более частный случай регрессионного анализа~-- \textit{нелинейная регрессия}.

	Идея построения нелинейной регрессии как и в случае с полиномиальной заключается в том, чтобы найти математическую функцию, которая максимально точно описывает зависимость между независимой переменной и зависимой от нее.
	Например, для построения нелинейной регрессии можно использовать функции типа полинома, логарифмической или экспоненциальной зависимости.

	В целом весь процесс нахождения нелинейной регрессионной модели можно поделить на два этапа:
	\begin{itemize}
		\item Определить регрессионную модель $f(w, x)$, которая зависит от параметров $w = (w_{1}, ~ \ldots, ~ w_{W})$ и свободной переменной $x$.
		\item Решить задачу по нахождению минимума сумма квадратов регрессионных остатков:
		\[
			S = \sum\limits_{i = 1}^{m}{r_{i}^{2}}, ~ r_{i} = y_{i} - f(w, x_{i})
		\]
	\end{itemize}
	Однако, решая в лоб такую задачу, мы сталкиваемся с оптимизационной задачи нахождения параметров нелинейной регрессионной модели.
	Тут к нам и приходят на помощь различные методы нахождения, в том числе и рассматриваемые ниже: \textit{Gauss-Newton} и \textit{Powell Dog Leg}.
	\subsection*{Gauss-Newton}
	Напомним, что мы решаем следующую задачу: дана нелинейная модель $f(w, x)$, где $w \in \mathbb{R}^{m}$, тогда сумма квадратов регрессионных остатков высчитывается как
	\[
		S = \sum\limits_{i = 1}^{\texttt{sizeof}~X}{(f(w, x_{i}) - y_{i})^2} \to \mathrm{min}
	\]
	Итак, пусть $n = \texttt{sizeof}~X$ и введем некоторые новые объекты для решения задачи, пусть $w^{0} = (w_{0}^{0}, ~ w_{1}^{0}, ~ \ldots, ~ w^{0}_{m})$~-- начальное приближение, и
	\begin{align*}
		\gimel &= \left(\dfrac{\partial{f}}{\partial{w_{j}}}{(w^{\mathbf{i}}, x_{i})}\right)_{n \times m}~-~\text{Якобиан, или матрица первых производных} \\
		\vec{f_{\mathbf{i}}} &= \left(f(w^{\mathbf{i}}, x_{i})\right)_{n \times 1}~-~\text{вектор значений функции}~f \\
		\eth_{\mathbf{i}} &= \texttt{const}~-~\text{размера шага}
	\end{align*}
	Тогда, формула $\mathbf{i}$-й итерации рассматриваемого метода будет высчитываться как
	\[
		w^{\mathbf{i} + 1} \gets w^{\mathbf{i}} - \eth_{\mathbf{i}} \cdot \underbrace{\left(\gimel^{\mathrm{T}}_{\mathbf{i}}\gimel_{\mathbf{i}}\right)^{-1}\gimel_{\mathbf{i}}^{\mathrm{T}}}_{\beta}(\vec{f_{\mathbf{i}}} - y),
	\] где $\beta$~-- это псевдообратная матрица к матрице $\gimel_{\mathbf{i}}$, или решение некоторой задачи многомерной линейной регрессии, где мы ищем такой вектор $\beta$, что
	\[
		\left\|\gimel_{\mathbf{i}}\beta - (\vec{f_{\mathbf{i}}} - y)\right\|^{2} \to \mathrm{min},
	\] где $y$~-- вектор правильных/настоящих ответов нашей модели.
	Получается, для решения задачи, мы, так называемую, \textit{невязку} пытаемся приблизить линейной комбинацией вектора из матрицы Якобиана так, что при следующем шаге итерации получить такой $w^{\mathbf{i} + 1}$, который бы сократил нам расстояние невязки.
	Причем, заметим, что на каждом шаге, задача будет новой, так как $\gimel_{\mathbf{i}}$ зависит от текущего приближения, чтобы решить задачу многомерной регрессии.

	Заметим, что здесь, по алгоритму, мы видим достаточно очевидное ограничение: $m \geqslant n$, в ином случае для $\gimel_{\mathbf{i}}^{\mathrm{T}}\gimel_{\mathbf{i}}$ не будет существовать обратной матрицы и, в следствии, решения к уравнению.
	% TODO: добавить идейный псевдокод к задаче.
	\subsubsection*{Исследования}
	% TODO: добавить исследования к Gauss-Newton.
	\subsection*{Powell Dog Leg}
	\textit{Trust-region method}~--- это метод решения оптимизационных задач, который основывается на вычислении региона, в котором квадратичная модель аппроксимирует целевую функцию.
	Сам этот метод представляет из себя смесь сразу двух алгоритмов, решающих задачу:
	\begin{itemize}
		\item Линейный поиск используется для определения направления поиска и дальнейшего нахождения оптимального шага вдоль выбранного вектора пути.
		\item Сам по себе trust-region используется для определения области вокруг текущей итерации, в котором модель достаточно аппроксимирует целевую функцию. Причем, стоит заметить, что для поиска следующего радиуса рассматриваемого региона также будет использоваться линейный поиск.
	\end{itemize}
	В общем случае Trust-region на каждой итерации решает следующую квадратичную задачу:
	\[
		\min_{p \in \mathbb{R}^{n}}{m_{k}(p)} = f_{k} + p^{\mathrm{T}}g_{k} + \dfrac{1}{2}p^{\mathrm{T}}B_{k}p,
	\] где $f_{k} = f(x_{k})$, $g_{k} = \nabla{f_{k}}$, $B_{k} = \nabla^{2}{f_{k}}$ и $\nabla_{k} > 0$~-- изменяющийся радиус региона, причем всё это, при условии, что $|p| \leqslant \nabla_{k}$.
	Заметим, что в таком простейшем виде мы получаем безусловно почти бесполезный алгоритм: он чрезвычайно медленный из-за появления $B_{k}$~-- Гессиана функции.
	С другой стороны, если он положительно определен и $|B_{k}^{-1}\nabla{f_{k}}| \leqslant \nabla_{k}$, то решение легко определить: $p_{k}^{B} = -B_{k}^{-1}\nabla_{k}$.
	Но, опять же, высчитывать еще и обратную матрицу~-- дело долгое и медленное, поэтому, начиная отсюда и до конца все лабораторной работы, мы будем то и дело пытаться приближать наши значения к реальным/по настоящему посчитанным значениям Гессиан-функции.

	Здесь мы рассмотрим один из методов оптимизации при аппроксимации квадратичной модели~-- \textit{Powell Dog Leg}.
	Начнем, пожалуй, с определения радиуса рассматриваемого доверительного региона: в алгоритме dogleg обычно выбирают основываясь на сходстве функции $m_{k}$ (та, что мы решаем изначально) и оригинальной функции $f$ на предыдущей итерации.
	Зададим $\rho_{k}$ следующим образом:
	\[
		\rho_{k} = \dfrac{f_{k} - f^{\star}_{k}}{m_{k}(0) - m_{k}(p_{k})},
	\] где $f^{\star}_{k} = f(x_{k} + p_{k})$.
	А теперь посмотрим на то, как именно лучше поменять шаг: в том случае, если $\rho_{k}$ меньше нуля, то это значит, что наша модель далека от функции и нужно обязательно уменьшить радиус; в том случае, изменение функции почти не изменилось и мы попали на границу региона, то есть смысл увеличить радиус; в ином другом случае~-- остается неизменным.
	\[
		\Delta_{k + 1} =
		\begin{cases}
			\dfrac{1}{4} \Delta_{k}, & \rho_{k} < \dfrac{1}{4} \\
			\min{(2\Delta_{k}, ~ \Delta_{\texttt{max}})}, & \rho_{k} > \dfrac{3}{4} \land \|p_{k}\| = \Delta_{k} \\
			\Delta_{k}, & \text{в ином другом случае}
		\end{cases}
	\]
	Наконец, начинается самое интересное со стороны Powell Dog Leg.
	Итак, мы находимся на некоторой точки нашей модели, есть подсчитанный $\Delta$-радиуса доверительного региона, и посмотрим на полный шаг $p^{B} = -B^{-1}g$.
	Если $p^{B}$ лежит в окружности региона, то мы можем его взять и более закончить алгоритм.
	В ином случае, рассмотрим анти-градиент $-g$ и попробуем вдоль нее поискать минимум квадратичной модели, то есть решить
	\[
		\min_{\|-\tau g\| \leqslant \Delta}{m(-\tau g)}
	\]
	Для её решения мы можем взять некую новую точку без каких-либо ограничений в направлении анти-градиента и найти минимум модели
	\[
		p^{U} = -\dfrac{g^{\mathrm{T}}g}{g^{\mathrm{T}}Bg}g
	\]
	Здесь снова две ситуации, где может находиться т. $p^{U}$:
	\begin{itemize}
		\item Если она находится вне рассматриваемой области, то мы можем взять точку на границе и шагнуть туда.
		\item Если же она находится в окружности, то построим отрезок $p^Up^B$ и начнем искать минимум вдоль этих двух линий $\left(\circled{\text{текущая}} \to p^U~\text{и}~p^U \to p^B\right)$.
	\end{itemize}
	Наконец, вдоль пути мы рассматриваем траекторию $\hat{p}(\tau)$
	\[
		\hat{p}(\tau) =
		\begin{cases}
			\tau p^U, & 0 \leqslant \tau \leqslant 1 \\
			p^U + (\tau - 1)(p^B - p^U), & 1 \leqslant \tau \leqslant 2
		\end{cases}
	\]
	Подытожим.
	Мы получили, на самом деле, в чем-то схожий на метод Гаусса-Ньютона алгоритм нахождения схождения, в частности, кстати, точка $p^B$~-- это то, куда бы шагнул метод Гаусса-Ньютона, но при этом, если эта точка удовлетворяет нашим потребностям, то мы действуем как Гаусс-Ньютон, в ином случае~-- чуть по другому.
	Причем под <<немного другим>> способом предполагается, на самом деле, хитрая комбинация Гаусса-Ньютона и градиентного спуска (так как при маленьком доверительном регионе мы пойдем по направлению, близкому градиентному спуску).
	% TODO: добавить идейный псевдокод к задаче.
	\subsubsection*{Исследования}
	% TODO: добавить исследования к Powell Dog Leg.
	\newpage
	\section*{BFGS}
	\textit{BFGS}, или Алгоритм Бройдена~- Флетчера~- Гольдфарба~- Шанно~-- это тоже оптимизационный итерационный алгоритм для нахождения локального экстремума для не представимых данных или функций в линейном/полиномиальном виде.

	Один из известных квазиньютоновских методов (то есть, тех, которые основаны на получении информации о кривизне функции).
	Как и в случае с Powell Dog Leg, данный метод, в отличии от многих квазиньютоновских, использует аналог довольно медлительного постоянного переопределения Гессиана функции.
	Но если предыдущий механизм никак не взаимодействовал с явным Гессианом, то BFGS, наоборот, ускоряет работу на порядок: ибо он не явно каждый раз высчитывает матрицу, а лишь приближает к ней значения, при этом посчитав по честному Гессиан лишь один раз.

	Рассмотрим идею этого алгоритма.
	Обозначим за $x_{i} = \{x_{i}^{0}, ~ x_{i}^{1}, ~ \ldots, ~ x_{i}^{n - 1}\}$~-- координата в пространстве, где $n$~-- размерность соответствующего пространства.
	Пусть дана нам некоторая функция $f(x)$ и, как обычно, решаем задачу оптимизации нахождения $\operatorname*{argmin}_{x}{f(x)}$.
	Тогда, зададим некую начальную точку $x_{0}$ и $H_{0} = B_{0}^{-1}$~-- начальное приближение, где $B_{0}^{-1}$~-- обратный Гессиан функции, который или может быть посчитан в точке $x_{0}$, или выбран как $\mathbf{I}$~-- обратная матрица.
	Наконец, сам алгоритм:
	\begin{enumerate}[1)]
		\setcounter{enumi}{-1}
		\item Пусть $k$~-- текущий номер итерации алгоритма.
		\item Находим точку, в направлении которой будем производить поиск, она определяется следующим образом
		\[
			p_{k} = -H_{k} \times \nabla{f_{k}},
		\] где здесь и далее $f_{k} = f(x_{k})$.
		\item Вычисляем $x_{k + 1}$ через рекуррентное соотношение следующего вида:
		\[
			x_{k + 1} = x_{k} + \alpha \cdot p_{k},
		\] где $\alpha$~-- коэффициент, удовлетворяющий условиям Вольфа, которые, напомню, выглядят вот так
		\begin{align*}
			f(x_{k} + \alpha \cdot p_{k}) &\leqslant f(x_{k}) + c_{1} \cdot \alpha \cdot \nabla{f^{T}_{k}p_{k}} \\
			\nabla{f(x_{k} + \alpha \cdot p_{k})^{T}p_{k}} &\geqslant c_{2} \cdot \nabla{f^{T}_{k}p_{k}}
		\end{align*}
		\item Теперь определим размер шага алгоритма после данной итерации и изменение градиента следующими соответствующими образами
		\begin{align*}
			s_{k} &= x_{k + 1} - x_{k} \\
			y_{k} &= \nabla{f_{k + 1}} - \nabla{f_{k}}
		\end{align*}
		\item Наконец, обновим Гессиан функции, зная, что $\lambda = \dfrac{1}{y_{k}^{\mathrm{T}}s_{k}} \in \mathbb{R}$
		\[
			H_{k + 1} = \left(\mathbf{I} - \lambda s_{k}y_{k}^{\mathrm{T}}\right)H_{k}\left(\mathbf{I} - \lambda y_{i} s_{k}^{\mathrm{T}}\right) + \lambda s_{k} s_{k}^{\mathrm{T}}
		\]
	\end{enumerate}
	% TODO: добавить идейный псевдокод к задаче.
	\subsection*{Исследования}
	% TODO: добавить исследования для BFGS.
	\newpage
	\section*{L-BFGS}
	\textit{L-BFGS}, или BFGS с ограниченной памятью~-- это оптимизационный алгоритм, который аппроксимирует оригинальный алгоритм BFGS с использованием заданного ограниченного объема памяти.

	L-BFGS как BFGS использует приближенную оценку Гессиана, при этом в явном виде посчитав только один раз, а все остальные шаги лишь преобразовывая.
	Проблема: BFGS хранит всегда $n \times n$ приближение к обратному Гессиану.
	Решение: хранить несколько векторов, которые неявно представляют приближение, представляющие из себя историю последних $m$ обновлений положения $\vec{x}$ и градиента $\nabla{f(\vec{x})}$.
	При этом, $m$ обычно выбирается небольшим ($m < 10$).

	Рассмотрим идею этого алгоритма.
	Во многом она будет совпадать с предыдущим, поэтому пропустим обозначения и перейдем сразу алгоритму.
	\begin{enumerate}[1)]
		\setcounter{enumi}{-1}
		\item Пусть $\mathbf{i}$~-- текущий номер итерации алгоритма, возьмем $g_{\mathbf{i}} = \nabla{f(x_{\mathbf{i}})}$.
		\item Также находим точку, в направлении которой будем производить поиск:
		\[
			p_{\mathbf{i}} = -H_{\mathbf{i}} \times \nabla{f_{\mathbf{i}}}
		\]
		\item Пусть мы сохранили $m$ обновлений вида:
		\begin{align*}
			s_{\mathbf{i}} &= x_{\mathbf{i} + 1} - x_{\mathbf{i}} \\
			y_{\mathbf{i}} &= g_{\mathbf{i} + 1} - g_{\mathbf{i}}
		\end{align*}
		\item Определим $\rho_{\mathbf{i}} = \dfrac{1}{y_{\mathbf{i}}^{\mathrm{T}}s_{\mathbf{i}}}$ и $H_{\mathbf{i}}^{0}$~-- <<начальная>> аппроксимация обратного гессиана, с которого начинается наша оценка на $\mathbf{i}$-ой итерации.
		Теперь, наконец, основная оптимизация: мы хотим оптимизировать основную рекуррентную формулу.
		\begin{itemize}
			\item Для данного $\mathbf{i}$ определим $\{q_{\mathbf{i} - m}, ~ q_{\mathbf{i} - m + 1}, ~ \ldots, ~ q_{\mathbf{i}}\}$, где
			\begin{align*}
				q_{\mathbf{i}} &= g_{\mathbf{i}} \\
				q_{i} &= \left(\mathbf{I} - \rho_{i}y_{i}s_{i}^{\mathrm{T}}\right)q_{i + 1} ~ \forall i \setminus \mathbf{i}
			\end{align*}
			\item Тогда рекурсивный алгоритм вычисления $q_{i}$ от $q_{i + 1}$ состоит в том, чтобы определить $\alpha_{i} \gets \rho_{i}s_{i}^{\mathrm{T}}q_{i + 1}$ и $q_{i} = q_{i + 1} - \alpha_{i}y_{i}$.
			\item Определим также $\{z_{\mathbf{i} - m}, ~ z_{\mathbf{i} - m + 1}, ~ \ldots, ~ z_{\mathbf{i}}\}$, где $\forall i ~ z_{i} \gets H_{i}q_{i}$.
			\subitem Существует еще один рекурсивный алгоритм вычисления этих векторов, который заключается в определении $z_{\mathbf{i} - m} \gets H_{\mathbf{i}}^{0}q_{\mathbf{i} - m}$, а затем рекурсивно определить $\beta_{i} \gets \rho_{i}y_{i}^{\mathrm{T}}z_{i}$ и $z_{i + 1} = z_{i} + (\alpha_{i} - \beta_{i})s_{i}$. Значение $z_{\mathbf{i}}$ тогда~-- наше направление восхождения.
		\end{itemize}
		Таким образом, мы можем вычислить направление спуска следующим образом:
		\begin{mylisting}
			$q \gets \nabla{f_{\mathbf{i}}}$;
			for $i \in [\mathbf{i} - 1, ~ \mathbf{i} - 2, ~ \ldots, ~ \mathbf{i} - m]$ do
				$\alpha \gets \rho_{i}s_{i}^{\mathrm{T}}q$;
				$q \gets q - \alpha_{i}y_{i}$;
			end (for)
			$r \gets H_{\mathbf{i}}^{0}q$;
			for $i \in [k - m, ~ k - m + 1, ~ \ldots, ~ k - 1]$ do
				$\beta \gets \rho_{i}y_{i}^{\mathrm{T}}r$;
				$r \gets r + s_{i}(\alpha_{i} - \beta)$;
			end (for)
			return $r \equiv H_{\mathbf{i}}\nabla{f_{\mathbf{i}}}$;
		\end{mylisting}
		\item Положим $H_{\mathbf{i}}^{0} = \gamma_{\mathbf{i}}\mathbf{I}$ следующим образом:
		\[
			\gamma_{\mathbf{i}} = \dfrac{s_{\mathbf{i} - 1}^{\mathrm{T}}y_{\mathbf{i} - 1}}{y_{\mathbf{i} - 1}^{\mathrm{T}}y_{\mathbf{i} - 1}}
		\]
	\end{enumerate}
	% TODO: доделать.
	% TODO: исправить, отрефакторить.
	% TODO: добавить идейный псевдокод к задаче.
	\subsection*{Исследования}
	% TODO: добавить исследования для L-BFGS.
\end{document}
