\documentclass[12pt, a4paper, oneside, final]{article}
\usepackage[margin = 1in, bottom = 1in]{geometry}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{xcolor, ulem, soulutf8, soul, fancyhdr, amsmath, amssymb, amsthm, svg, wrapfig, csvsimple, float, caption, subcaption}
\usepackage[shortlabels]{enumitem}
\usepackage{titlesec, hyperref, multicol, listings}
\usepackage[most]{tcolorbox}
\usepackage[makeroom]{cancel}
\usepackage[most]{tcolorbox}
\usepackage{tocloft, longtable, skak, stmaryrd, color}
\usepackage[framemethod = tikz]{mdframed}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{blueish}{rgb}{0.96,0.96,1.0}
\definecolor{grayblueish}{rgb}{0.97,0.97,0.98}
\definecolor{transblue}{rgb}{0.9,0.9,0.97}
\definecolor{transred}{rgb}{0.97,0.9,0.9}
\definecolor{light-gray}{gray}{0.95}

\renewcommand{\figurename}{}
\addto\captionsrussian{\renewcommand{\figurename}{}}
\captionsetup[table]{labelformat = empty}

\hypersetup{
	colorlinks,
	citecolor = pink,
	filecolor = pink,
	linkcolor = pink,
	urlcolor = pink
}

\newtcblisting{mylisting}{
	listing only,
	breakable,
	colback = backcolour,
	enhanced jigsaw,
	sharp corners,
	boxrule = 0pt,
	frame hidden,
	listing options = {
		mathescape,
		commentstyle = \color{codegreen},
		keywordstyle = \color{magenta},
		numberstyle = \tiny\color{codegray},
		stringstyle = \color{codepurple},
		basicstyle = \ttfamily\footnotesize,
		breakatwhitespace = false,
		breaklines = true,
		captionpos = b,
		keepspaces = true,
		numbers = left,
		numbersep = 5pt,
		showspaces = false,
		showstringspaces = false,
		showtabs = false,
		tabsize = 4,
		inputencoding = utf8,
		language = python
	}
}

\binoppenalty = 10000
\relpenalty = 10000
\sloppy

\renewcommand*{\theenumi}{\thesection.\arabic{enumi}}
\renewcommand*{\theenumii}{\alph{enumii}}
\renewcommand*{\labelitemi}{\ensuremath{\triangleright}}

\everymath{\displaystyle}

\begin{document}
	\thispagestyle{empty}
	\vspace*{0.5em}
	\begin{center}
		{Национальный исследовательский университет ИТМО\\Факультет информационных технологий и программирования\\Прикладная математика и информатика}\\[5.0em]
		{\Huge \bfseries Методы оптимизации}\\[0.5em]
		{\large Отчет по лабораторной работе №3}\\[0.5em]
		\textcolor{gray}{\textlangle Собрано \today\textrangle}
	\end{center}
	\begingroup
	\def\hd{\begin{tabular}{ll}
			\textbf{Работу выполнили:} \\ Бактурин Савелий Филиппович M32331 \\ Вереня Андрей Тарасович M32331 \\ Сотников Максим Владимирович M32331 \vspace*{1em} \\
			\textbf{Преподаватель:} \\ Ким Станислав Евгеньевич
		\end{tabular}
	}
	\vspace*{30em}
	\newlength{\hdwidth}
	\settowidth{\hdwidth}{\hd}
	\hfill\begin{minipage}{\hdwidth}\hd\end{minipage}
	\endgroup
	\newpage
	\section*{Решение задачи нелинейной регрессии}
	Часто решая задачу создания регрессионной модели мы сталкиваемся с тем, что по жизни очень немногие рассматриваемые функции оказываются не представимы в виде обобщенной линейной зависимости или полиномиальной некоторой конечной степени $k$.
	Такая же ситуация часто случается и с некоторым набором данных, который нужно как-то обобщить.
	Именно в таких случаях к нам на помощь приходит более частный случай регрессионного анализа~-- \textit{нелинейная регрессия}.

	Идея построения нелинейной регрессии как и в случае с полиномиальной заключается в том, чтобы найти математическую функцию, которая максимально точно описывает зависимость между независимой переменной и зависимой от нее.
	Например, для построения нелинейной регрессии можно использовать функции типа полинома, логарифмической или экспоненциальной зависимости.

	В целом весь процесс нахождения нелинейной регрессионной модели можно поделить на два этапа:
	\begin{itemize}
		\item Определить регрессионную модель $f(w, x)$, которая зависит от параметров $w = (w_{1}, ~ \ldots, ~ w_{W})$ и свободной переменной $x$.
		\item Решить задачу по нахождению минимума сумма квадратов регрессионных остатков:
		\[
			S = \sum\limits_{i = 1}^{m}{r_{i}^{2}}, ~ r_{i} = y_{i} - f(w, x_{i})
		\]
	\end{itemize}
	Однако, решая в лоб такую задачу, мы сталкиваемся с оптимизационной задачи нахождения параметров нелинейной регрессионной модели.
	Тут к нам и приходят на помощь различные методы нахождения, в том числе и рассматриваемые ниже: \textit{Gauss-Newton} и \textit{Powell Dog Leg}.
	\subsection*{Gauss-Newton}
	Напомним, что мы решаем следующую задачу: дана нелинейная модель $f(w, x)$, где $w \in \mathbb{R}^{n}$, тогда сумма квадратов регрессионных остатков высчитывается как
	\[
		S = \sum\limits_{i = 1}^{\texttt{sizeof}~X}{(f(w, x_{i}) - y_{i})^2} \to \mathrm{min}
	\]
	Итак, теперь введем некоторые новые объекты для решения задачи, пусть $w^{0} = (w_{0}^{0}, ~ w_{1}^{0}, ~ \ldots, ~ w^{t}_{p})$~-- начальное приближение, и
	\begin{align*}
		\gimel &= \left(\dfrac{\partial{f}}{\partial{w_{j}}}{(w^{t}, x_{i})}\right)_{(\texttt{sizeof}~X) \times p}~-~\text{Якобиан, или матрица первых производных} \\
		\vec{f_{t}} &= \left(f(w^{t}, x_{i})\right)_{(\texttt{sizeof}~X) \times 1}~-~\text{вектор значений}~f \\
		\eth_{t} &= \texttt{const}~-~\text{размера шага}
	\end{align*}
	Тогда, формула $t$-й итерации рассматриваемого метода будет высчитываться как
	\[
		w^{t + 1} \gets w^{t} - \eth_{t} \cdot \underbrace{\left(\gimel^{\mathrm{T}}_{t}\gimel_{t}\right)^{-1}\gimel_{t}^{\mathrm{T}}}_{\Psi}(\vec{f_{t}} - y),
	\] где $\Psi$~-- это псевдообратная матрица, или решение некоторой задачи многомерной линейной регрессии, где мы ищем такой вектор $\Psi$, что
	\[
		\left\|\gimel_{t}\Psi - (\vec{f_{t}} - y)\right\|^{2} \to \mathrm{min},
	\] где $y$~-- вектор правильных/настоящих ответов нашей модели.
	Получается, для решения задачи, мы, так называемую, \textit{невязку} пытаемся приблизить линейной комбинацией вектора из матрицы Якобиана так, что при следующем шаге итерации получить такой $w^{t + 1}$, который бы сократил нам расстояние невязки.
	Причем, заметим, что на каждом шаге, задача будет новой, так как $\gimel_{t}$ зависит от текущего приближения, чтобы решить задачу многомерной регрессии.
	% TODO: надо ли ещё информации?
	\subsubsection*{Исследования}
	% TODO: добавить исследования из T1.ipynb
	\subsection*{Powell Dog Leg}
	% TODO: добавить информации для dogleg.
	\subsubsection*{Исследования}
	\newpage
	\section*{BFGS}
	\textit{BFGS}, или Алгоритм Бройдена~- Флетчера~- Гольдфарба~- Шанно~-- это тоже оптимизационный итерационный алгоритм для нахождения локального экстремума для не представимых данных или функций в линейном/полиномиальном виде.

	Один из известных квазиньютоновских методов (то есть, тех, которые основаны на получении информации о кривизне функции).
	Тут следует сразу пояснить, что в квазиньютоновских методах для нахождения оптимальных параметров используется довольно медлительное определение \textit{гессиана} функции (или: матрица вторых производных).
	И вот тут данный алгоритм ускоряет работу на порядок: ибо он не явно каждый раз высчитывает матрицу, а лишь приближает к ней значения.

	Рассмотрим идею этого алгоритма.
	Пусть дана нам некоторая функция $f(\vec{x})$ и, как обычно, решаем задачу оптимизации нахождения $\operatorname*{argmin}_{\vec{x}}{f(\vec{x})}$.
	Пусть также $x_{i} = \{x_{i}^{0}, ~ x_{i}^{1}, ~ \ldots, ~ x_{i}^{n - 1}\}$, где $n$~-- размерность рассматриваемого пространства; $x_{0} \gets \textbf{INIT}$~--- начальная точка; $H_{0} = B_{0}^{-1}$~--- начальное приближение, где $B_{0}^{-1}$~-- обратный гессиан функции.
	Тогда:
	\begin{enumerate}[1)]
		\setcounter{enumi}{-1}
		\item Пусть $i$~-- текущий номер итерации алгоритма.
		\item Находим точку, в направлении которой будем производить поиск, она определяется следующим образом:
		\[
			p_{i} = -H_{i} \times \nabla{f_{i}}
		\]
		\item Вычисляем $x_{i + 1}$ через рекуррентное соотношение следующего вида:
		\[
			x_{i + 1} = x_{i} + \alpha \cdot p_{i},
		\] где $\alpha$~-- коэффициент, удовлетворяющий условиям Вольфа, которые, напомню, выглядят вот так:
		\begin{align*}
			f(x_{i} + \alpha \cdot p_{i}) &\leqslant f(x_{i}) + c_{1} \cdot \alpha \cdot \nabla{f^{T}_{k}p_{i}} \\
			\nabla{f(x_{i} + \alpha \cdot p_{i})^{T}p_{i}} &\geqslant c_{2} \cdot \nabla{f^{T}_{i}p_{i}}
		\end{align*}
		\item Теперь определим размер шага алгоритма после данной итерации и изменение градиента следующими соответствующими образами:
		\begin{align*}
			s_{i} &= x_{i + 1} - x_{i} \\
			y_{i} &= \nabla{f_{i + 1}} - \nabla{f_{i}}
		\end{align*}
		\item Наконец, обновим гессиан функции, зная, что $\mathbf{I}$~-- единичная матрица и $\lambda = \dfrac{1}{y_{i}^{\mathrm{T}}s_{i}}$:
		\[
			H_{i + 1} = \left(\mathbf{I} - \lambda s_{i}y_{i}^{\mathrm{T}}\right)H_{i}\left(\mathbf{I} - \lambda y_{i} s_{i}^{\mathrm{T}}\right) + \lambda s_{i} s_{i}^{\mathrm{T}}
		\]
	\end{enumerate}
	% TODO: надо ли ещё информации?
	\subsection*{Исследования}
	% TODO: добавить исследования для bfgs.
	\newpage
	\section*{L-BFGS}
	\textit{L-BFGS}, или BFGS с ограниченной памятью~-- это оптимизационный алгоритм, который аппроксимирует оригинальный алгоритм BFGS с использованием заданного ограниченного объема памяти.

	L-BFGS как BFGS использует приближенную оценку Гессиана, при этом в явном виде посчитав только один раз, а все остальные шаги лишь преобразовывая.
	Проблема: BFGS хранит всегда $n \times n$ приближение к обратному Гессиану.
	Решение: хранить несколько векторов, которые неявно представляют приближение, представляющие из себя историю последних $m$ обновлений положения $\vec{x}$ и градиента $\nabla{f(\vec{x})}$.
	При этом, $m$ обычно выбирается небольшим ($m < 10$).

	Рассмотрим идею этого алгоритма.
	Во многом она будет совпадать с предыдущим, поэтому пропустим обозначения и перейдем сразу алгоритму.
	\begin{enumerate}[1)]
		\setcounter{enumi}{-1}
		\item Пусть $i$~-- текущий номер итерации алгоритма, возьмем $g_{i} = \nabla{f(x_{i})}$.
		\item Также находим точку, в направлении которой будем производить поиск:
		\[
			p_{i} = -H_{i} \times \nabla{f_{i}}
		\]
		\item Пусть мы сохранили $m$ обновлений вида:
		\begin{align*}
			s_{i} &= x_{i + 1} - x_{i} \\
			y_{i} &= g_{i + 1} - g_{i}
		\end{align*}
		\item Определим $\rho_{i} = \dfrac{1}{y_{i}^{\mathrm{T}}s_{i}}$ и $H_{i}^{0}$~-- <<начальная>> аппроксимация обратного гессиана, с которого начинается наша оценка на $i$-ой итерации.
		Теперь, наконец, основная оптимизация: мы хотим оптимизировать основную рекуррентную формулу.
		\begin{itemize}
			\item Для данного $i$ определим $\{q_{i - m}, ~ q_{i - m + 1}, ~ \ldots, ~ q_{i}\}$, где
			\begin{align*}
				q_{i} &= g_{i} \\
				q_{j} &= \left(\mathbf{I} - \rho_{j}y_{j}s_{j}^{\mathrm{T}}\right)q_{j + 1} ~ \forall j \setminus i
			\end{align*}
			\item Тогда рекурсивный алгоритм вычисления $q_{j}$ от $q_{j + 1}$ состоит в том, чтобы определить $\alpha_{j} \gets \rho_{j}s_{j}^{\mathrm{T}}q_{j + 1}$ и $q_{j} = q_{j + 1} - \alpha_{j}y_{j}$.
			\item Определим также $\{z_{i - m}, ~ z_{i - m + 1}, ~ \ldots, ~ z_{i}\}$, где $\forall j ~ z_{j} \gets H_{j}q_{j}$.
			\subitem Существует еще один рекурсивный алгоритм вычисления этих векторов, который заключается в определении $z_{i - m} \gets H_{i}^{0}q_{i - m}$, а затем рекурсивно определить $\beta_{j} \gets \rho_{j}y_{j}^{\mathrm{T}}z_{j}$ и $z_{j + 1} = z_{j} + (\alpha_{j} - \beta_{j})s_{j}$. Значение $z_{i}$ тогда~-- наше направление восхождения.
		\end{itemize}
		Таким образом, мы можем вычислить направление спуска следующим образом:
		\begin{mylisting}
			$q \gets g_{i}$
			for $j \gets i - 1, ~ \ldots, ~ i - m$ do
				$\alpha_{j} \gets \rho_{j}s_{j}^{\mathrm{T}}q$
				$q \gets q - \alpha_{j}y_{j}$
			$\gamma_{i} \gets \dfrac{s_{i - 1}^{\mathrm{T}}y_{i - 1}}{y^{\mathrm{T}}_{i - 1}y_{i - 1}}$
			$H_{i}^{0} \gets \gamma_{i}\mathbf{I}$
			$z \gets H_{i}^{0}q$
			for $j \gets i - m, ~ \ldots, ~ i - 1$ do
				$\beta_{j} \gets \rho_{j}y_{j}^{\mathrm{T}}z$
				$z \gets z + s_{j}(\alpha_{j} - \beta_{j})$
			$z \gets -z$
		\end{mylisting}
	\end{enumerate}
	% TODO: добавить больше информации.
	\subsection*{Исследования}
	% TODO: добавить исследования для l-bfgs.
\end{document}
